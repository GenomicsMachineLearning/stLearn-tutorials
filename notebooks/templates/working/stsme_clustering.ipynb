{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stSME clustering tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**stSME** is a novel normalisation method implemented in *stLearn* software.   \n",
    "It's designed for **s**patial **t**ranscriptomics data and utilised tissue **S**patial location, **M**orphology, , and gene **E**xpression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to use *stLearn* to perform **stSME** clustering for spatial transcriptomics data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we first focus on **Mouse Brain (Coronal)** Visium dataset from [10x genomics website](https://support.10xgenomics.com/spatial-gene-expression/datasets/1.0.0/V1_Adult_Mouse_Brain)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mouse Brain (Coronal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# import module\n",
    "import stlearn as st\n",
    "from pathlib import Path\n",
    "st.settings.set_figure_params(dpi=180)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# specify PATH to data\n",
    "BASE_PATH = Path(\"/home/uqysun19/60days/10x_visium/mouse_brain_coronal\")\n",
    "\n",
    "# spot tile is the intermediate result of image pre-processing\n",
    "TILE_PATH = Path(\"/tmp/tiles\")\n",
    "TILE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# output path\n",
    "OUT_PATH = Path(\"/home/uqysun19/60days/stlearn_plot/mouse_brain_coronl\")\n",
    "OUT_PATH.mkdir(parents=True, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# load data\n",
    "data = st.Read10X(BASE_PATH)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# pre-processing for gene count table\n",
    "st.pp.filter_genes(data,min_cells=1)\n",
    "st.pp.normalize_total(data)\n",
    "st.pp.log1p(data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# pre-processing for spot image\n",
    "st.pp.tiling(data, TILE_PATH)\n",
    "\n",
    "# this step uses deep learning model to extract high-level features from tile images\n",
    "# may need few minutes to be completed\n",
    "st.pp.extract_feature(data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. run stSME clustering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# run PCA for gene expression data\n",
    "st.em.run_pca(data,n_comps=50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_SME = data.copy()\n",
    "# apply stSME to normalise log transformed data\n",
    "st.spatial.SME.SME_normalize(data_SME, use_data=\"raw\")\n",
    "data_SME.X = data_SME.obsm['raw_SME_normalized']\n",
    "st.pp.scale(data_SME)\n",
    "st.em.run_pca(data_SME,n_comps=50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# K-means clustering on stSME normalised PCA\n",
    "st.tl.clustering.kmeans(data_SME,n_clusters=19, use_data=\"X_pca\", key_added=\"X_pca_kmeans\")\n",
    "st.pl.cluster_plot(data_SME, use_label=\"X_pca_kmeans\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# louvain clustering on stSME normalised data\n",
    "st.pp.neighbors(data_SME,n_neighbors=17,use_rep='X_pca')\n",
    "st.tl.clustering.louvain(data_SME, resolution=1.19)\n",
    "st.pl.cluster_plot(data_SME,use_label=\"louvain\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we now move to Mouse Brain (Sagittal Posterior) Visium dataset from [10x genomics website](https://support.10xgenomics.com/spatial-gene-expression/datasets/1.0.0/V1_Mouse_Brain_Sagittal_Posterior)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mouse Brain (Sagittal Posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# specify PATH to data\n",
    "BASE_PATH = Path(\"/home/uqysun19/60days/10x_visium/mouse_brain_s_p_1/\")\n",
    "\n",
    "# spot tile is the intermediate result of image pre-processing\n",
    "TILE_PATH = Path(\"/tmp/tiles\")\n",
    "TILE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# outpot path\n",
    "OUT_PATH = Path(\"/home/uqysun19/60days/stlearn_plot/mouse_brain_s_p_1/\")\n",
    "OUT_PATH.mkdir(parents=True, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# load data\n",
    "data = st.Read10X(BASE_PATH)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# pre-processing for gene count table\n",
    "st.pp.filter_genes(data,min_cells=1)\n",
    "st.pp.normalize_total(data)\n",
    "st.pp.log1p(data)\n",
    "st.pp.scale(data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# pre-processing for spot image\n",
    "st.pp.tiling(data, TILE_PATH)\n",
    "\n",
    "# this step uses deep learning model to extract high-level features from tile images\n",
    "# may need few minutes to be completed\n",
    "st.pp.extract_feature(data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. run stSME clustering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# run PCA for gene expression data\n",
    "st.em.run_pca(data,n_comps=50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_SME = data.copy()\n",
    "# apply stSME to normalise log transformed data\n",
    "# with weights from morphological Similarly and physcial distance\n",
    "st.spatial.SME.SME_normalize(data_SME, use_data=\"raw\", \n",
    "                             weights=\"weights_matrix_pd_md\")\n",
    "data_SME.X = data_SME.obsm['raw_SME_normalized']\n",
    "st.pp.scale(data_SME)\n",
    "st.em.run_pca(data_SME,n_comps=50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# K-means clustering on stSME normalised PCA\n",
    "st.tl.clustering.kmeans(data_SME,n_clusters=17, use_data=\"X_pca\", key_added=\"X_pca_kmeans\")\n",
    "st.pl.cluster_plot(data_SME, use_label=\"X_pca_kmeans\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# louvain clustering on stSME normalised data\n",
    "st.pp.neighbors(data_SME,n_neighbors=20,use_rep='X_pca')\n",
    "st.tl.clustering.louvain(data_SME)\n",
    "st.pl.cluster_plot(data_SME,use_label=\"louvain\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply stSME clustering on Human Brain dorsolateral prefrontal cortex (DLPFC) Visium dataset from this [paper](https://www.biorxiv.org/content/10.1101/2020.02.28.969931v1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Human Brain dorsolateral prefrontal cortex (DLPFC)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, silhouette_score, \\\n",
    "                            homogeneity_completeness_v_measure\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import scanpy "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    cm = contingency_matrix(y_true, y_pred)\n",
    "    # return purity\n",
    "    return np.sum(np.amax(cm, axis=0)) / np.sum(cm)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# specify PATH to data\n",
    "BASE_PATH = Path(\"/home/uqysun19/60days/Human_Brain_spatialLIBD\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# here we include all 12 samples\n",
    "sample_list = [\"151507\", \"151508\", \"151509\",\n",
    "               \"151510\", \"151669\", \"151670\",\n",
    "               \"151671\", \"151672\", \"151673\",\n",
    "               \"151674\", \"151675\", \"151676\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for i in range(len(sample_list)):\n",
    "    sample = sample_list[i]\n",
    "\n",
    "    GROUND_TRUTH_PATH = BASE_PATH / sample / \"cluster_labels_{}.csv\".format(sample)\n",
    "    ground_truth_df = pd.read_csv(GROUND_TRUTH_PATH, sep=',', index_col=0)\n",
    "    ground_truth_df.index = ground_truth_df.index.map(lambda x: x[7:])\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    ground_truth_le = le.fit_transform(list(ground_truth_df[\"ground_truth\"].values))\n",
    "    ground_truth_df[\"ground_truth_le\"] = ground_truth_le\n",
    "    \n",
    "    # load data\n",
    "    data = st.Read10X(BASE_PATH / sample)\n",
    "    ground_truth_df = ground_truth_df.reindex(data.obs_names)\n",
    "    data.obs[\"ground_truth\"] = pd.Categorical(ground_truth_df[\"ground_truth\"])\n",
    "    st.pl.cluster_plot(data, use_label=\"ground_truth\", cell_alpha=0.5)\n",
    "    \n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_clustering_matrix(pred, gt, sample, methods_):\n",
    "    df = pd.DataFrame(columns=['Sample', 'Score', 'PCA_or_UMAP', 'Method', \"test\"])\n",
    "    \n",
    "    pca_ari = adjusted_rand_score(pred, gt)\n",
    "    df = df.append(pd.Series([sample, pca_ari, \"pca\", methods_, \"Adjusted_Rand_Score\"],\n",
    "                             index=['Sample', 'Score', 'PCA_or_UMAP', 'Method', \"test\"]), ignore_index=True)\n",
    "   \n",
    "    pca_nmi = normalized_mutual_info_score(pred, gt)\n",
    "    df = df.append(pd.Series([sample, pca_nmi, \"pca\", methods_, \"Normalized_Mutual_Info_Score\"],\n",
    "                             index=['Sample', 'Score', 'PCA_or_UMAP', 'Method', \"test\"]), ignore_index=True)\n",
    "    \n",
    "    pca_purity = purity_score(pred, gt)\n",
    "    df = df.append(pd.Series([sample, pca_purity, \"pca\", methods_, \"Purity_Score\"],\n",
    "                             index=['Sample', 'Score', 'PCA_or_UMAP', 'Method', \"test\"]), ignore_index=True)\n",
    "    \n",
    "    pca_homogeneity, pca_completeness, pca_v_measure = homogeneity_completeness_v_measure(pred, gt)\n",
    "    \n",
    "    df = df.append(pd.Series([sample, pca_homogeneity, \"pca\", methods_, \"Homogeneity_Score\"],\n",
    "                             index=['Sample', 'Score', 'PCA_or_UMAP', 'Method', \"test\"]), ignore_index=True)\n",
    "\n",
    "    \n",
    "    df = df.append(pd.Series([sample, pca_completeness, \"pca\", methods_, \"Completeness_Score\"],\n",
    "                             index=['Sample', 'Score', 'PCA_or_UMAP', 'Method', \"test\"]), ignore_index=True)\n",
    " \n",
    "    df = df.append(pd.Series([sample, pca_v_measure, \"pca\", methods_, \"V_Measure_Score\"],\n",
    "                             index=['Sample', 'Score', 'PCA_or_UMAP', 'Method', \"test\"]), ignore_index=True)\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = pd.DataFrame(columns=['Sample', 'Score', 'PCA_or_UMAP', 'Method', \"test\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for i in range(12):\n",
    "    sample = sample_list[i]\n",
    "    GROUND_TRUTH_PATH = BASE_PATH / sample / \"cluster_labels_{}.csv\".format(sample)\n",
    "    ground_truth_df = pd.read_csv(GROUND_TRUTH_PATH, sep=',', index_col=0)\n",
    "    ground_truth_df.index = ground_truth_df.index.map(lambda x: x[7:])\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    ground_truth_le = le.fit_transform(list(ground_truth_df[\"ground_truth\"].values))\n",
    "    ground_truth_df[\"ground_truth_le\"] = ground_truth_le\n",
    "    TILE_PATH = Path(\"/tmp/{}_tiles\".format(sample))\n",
    "    TILE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    data = st.Read10X(BASE_PATH / sample)\n",
    "    ground_truth_df = ground_truth_df.reindex(data.obs_names)\n",
    "    n_cluster = len((set(ground_truth_df[\"ground_truth\"]))) - 1\n",
    "    data.obs['ground_truth'] = ground_truth_df[\"ground_truth\"]\n",
    "    ground_truth_le = ground_truth_df[\"ground_truth_le\"]\n",
    "    \n",
    "    # pre-processing for gene count table\n",
    "    st.pp.filter_genes(data,min_cells=1)\n",
    "    st.pp.normalize_total(data)\n",
    "    st.pp.log1p(data)\n",
    "    \n",
    "    # run PCA for gene expression data\n",
    "    st.em.run_pca(data,n_comps=15)\n",
    "    \n",
    "    # pre-processing for spot image\n",
    "    st.pp.tiling(data, TILE_PATH)\n",
    "\n",
    "    # this step uses deep learning model to extract high-level features from tile images\n",
    "    # may need few minutes to be completed\n",
    "    st.pp.extract_feature(data)\n",
    "    \n",
    "    # stSME\n",
    "    st.spatial.SME.SME_normalize(data, use_data=\"raw\", weights=\"physical_distance\")\n",
    "    data_ = data.copy()\n",
    "    data_.X = data_.obsm['raw_SME_normalized']\n",
    "    \n",
    "    st.pp.scale(data_)\n",
    "    st.em.run_pca(data_,n_comps=15)\n",
    "    \n",
    "    st.tl.clustering.kmeans(data_, n_clusters=n_cluster, use_data=\"X_pca\", key_added=\"X_pca_kmeans\")\n",
    "    st.pl.cluster_plot(data_, use_label=\"X_pca_kmeans\")\n",
    "    \n",
    "    methods_ = \"stSME_disk\"\n",
    "    results_df = calculate_clustering_matrix(data_.obs[\"X_pca_kmeans\"], ground_truth_le, sample, methods_)\n",
    "    df = df.append(results_df, ignore_index=True)\n",
    "    \n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# read clustering results from other methods\n",
    "pca_df = pd.read_csv(\"./stSME_matrices_other_methods.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_all = pca_df.append(df, ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style(\"white\")\n",
    "from matplotlib.patches import PathPatch\n",
    "\n",
    "def adjust_box_widths(g, fac):\n",
    "    \"\"\"\n",
    "    Adjust the widths of a seaborn-generated boxplot.\n",
    "    \"\"\"\n",
    "\n",
    "    # iterating through Axes instances\n",
    "    for ax in g.axes:\n",
    "\n",
    "        # iterating through axes artists:\n",
    "        for c in ax.get_children():\n",
    "\n",
    "            # searching for PathPatches\n",
    "            if isinstance(c, PathPatch):\n",
    "                # getting current width of box:\n",
    "                p = c.get_path()\n",
    "                verts = p.vertices\n",
    "                verts_sub = verts[:-1]\n",
    "                xmin = np.min(verts_sub[:, 0])\n",
    "                xmax = np.max(verts_sub[:, 0])\n",
    "                xmid = 0.5*(xmin+xmax)\n",
    "                xhalf = 0.5*(xmax - xmin)\n",
    "\n",
    "                # setting new width of box\n",
    "                xmin_new = xmid-fac*xhalf\n",
    "                xmax_new = xmid+fac*xhalf\n",
    "                verts_sub[verts_sub[:, 0] == xmin, 0] = xmin_new\n",
    "                verts_sub[verts_sub[:, 0] == xmax, 0] = xmax_new\n",
    "\n",
    "                # setting new width of median line\n",
    "                for l in ax.lines:\n",
    "                    if np.all(l.get_xdata() == [xmin, xmax]):\n",
    "                        l.set_xdata([xmin_new, xmax_new])\n",
    "\n",
    "\n",
    "class GridShader():\n",
    "    def __init__(self, ax, first=True, **kwargs):\n",
    "        self.spans = []\n",
    "        self.sf = first\n",
    "        self.ax = ax\n",
    "        self.kw = kwargs\n",
    "        self.ax.autoscale(False, axis=\"x\")\n",
    "        self.cid = self.ax.callbacks.connect('xlim_changed', self.shade)\n",
    "        self.shade()\n",
    "    def clear(self):\n",
    "        for span in self.spans:\n",
    "            try:\n",
    "                span.remove()\n",
    "            except:\n",
    "                pass\n",
    "    def shade(self, evt=None):\n",
    "        self.clear()\n",
    "        xticks = self.ax.get_xticks()\n",
    "        xlim = self.ax.get_xlim()\n",
    "        xticks = xticks[(xticks > xlim[0]) & (xticks < xlim[-1])]\n",
    "        locs = np.concatenate(([[xlim[0]], xticks+0.5, [xlim[-1]]]))\n",
    "        \n",
    "        start = locs[1-int(self.sf)::2]  \n",
    "        end = locs[2-int(self.sf)::2]\n",
    "\n",
    "        for s, e in zip(start, end):\n",
    "            self.spans.append(self.ax.axvspan(s, e, zorder=0, **self.kw))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "a = sns.boxplot(x=\"Method\", y=\"Score\", hue=\"test\",\n",
    "\n",
    "               width=0.7,\n",
    "               data=df_all)\n",
    "a.set_xticklabels(a.get_xticklabels(), rotation=45)\n",
    "sns.despine(left=True)\n",
    "a.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "adjust_box_widths(fig, 0.7)\n",
    "plt.autoscale()\n",
    "gs = GridShader(a, facecolor=\"lightgrey\", first=False, alpha=0.7)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"./clustering_performace.png\", dpi=300)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tutorial by Xiao Tan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
